[MINKLOC3D-SI]
backbone = MinkFPN
pooling = GeM
version = MinkLoc3D
mink_quantization_size = 0.01, 0.01, 0.01
planes = 32,64,64
layers = 1,1,1
num_top_down = 1
conv0_kernel_size = 5
feature_size = 256
output_dim = 256
gpu = 0

[POINTNET]
with_pntnet = False
pnt2s = True

[SELF-ATTENTION]
with_self_att = True
num_layers = 6
linear_att = False
kernel_size = 3
stride = 1
dilation = 1
num_heads = 8

[POINTNET-CROSS-ATTENTION]
with_cross_att = False
pnt2s = True
num_heads = 2
d_feedforward = 64
dropout = 0
transformer_act = relu
pre_norm = True
attention_type = linear_attention
sa_val_has_pos_emb = True
ca_val_has_pos_emb = True
num_encoder_layers = 2
transformer_encoder_has_pos_emb = True

