[MINKLOC3D-SI]
backbone = MinkFPN
pooling = GeM
version = MinkLoc3D-S
mink_quantization_size = 2.5,2.0,1.875
planes = 32,64,64
layers = 1,1,1
num_top_down = 1
conv0_kernel_size = 5
feature_size = 256
output_dim = 256
gpu = 0

[FCGF]
in_channels = 1
out_channels = 256
bn_momentum = 0.1
normalize_feature = True
conv1_kernel_size = 5
D = 3

[POINTNET]
with_pntnet = False
pnt2s = True

[SELF-ATTENTION]
with_self_att = True
num_layers = 3
linear_att = False
kernel_size = 3
stride = 1
dilation = 1
num_heads = 8

[POINTNET-CROSS-ATTENTION]
with_cross_att = True
pnt2s = True
num_heads = 2
d_feedforward = 64
dropout = 0
transformer_act = relu
pre_norm = True
attention_type = dot_prod
sa_val_has_pos_emb = True
ca_val_has_pos_emb = True
num_encoder_layers = 2
transformer_encoder_has_pos_emb = True

[MULTI-CROSS-ATTENTION]
with_multi_att = False
num_heads = 2
d_feedforward = 64
dropout = 0
transformer_act = relu
pre_norm = True
attention_type = dot_prod
sa_val_has_pos_emb = True
ca_val_has_pos_emb = True
num_encoder_layers = 2
transformer_encoder_has_pos_emb = True
